from torchvision import transforms, models
from torchvision.models import resnet50
import torch.nn as nn
import torch.nn.functional as F



class Resnet50Model(nn.Module):
    def __init__(self, pretrained=True):
        super(Resnet50Model, self).__init__()
        self.pretrained = pretrained
        self.model = self._load_model()
        for param in self.model.parameters():
            param.requires_grad = False
    # Unfreeze only the final layer
        for param in self.model.fc.parameters():
            param.requires_grad = True

    def _load_model(self):
        # Placeholder for model loading logic
        if self.pretrained:
            print("Loading pretrained ResNet50 model...")
            resnet50_model = resnet50(weights='DEFAULT')
        else:
            print("Loading ResNet50 model without pretrained weights...")
            resnet50_model = resnet50(weights=None)
        # Replace the final FC layer for binary classification (single output)
        resnet50_model.fc = nn.Linear(resnet50_model.fc.in_features, 1)
        return resnet50_model

    def forward(self, x):
        return self.model(x)
    
    def gradient_embedding(self, x):
        """
        This is unchecked code that computes gradient embeddings generated by chatGPT.
        This is for the BADGE sampling method.
        """
        x = x.clone().detach().requires_grad_(True)
        logits = self.forward(x)

        # Generate pseudo-labels from model predictions
        pseudo_labels = (logits > 0).float()

        # Compute loss with respect to pseudo-labels
        loss = F.binary_cross_entropy_with_logits(logits, pseudo_labels, reduction='sum')

        # Backpropagate to get gradients
        loss.backward()

        # Extract gradients as embeddings
        embeddings = x.grad.view(x.size(0), -1).cpu().numpy()

        # Clear gradients to avoid accumulation
        x.grad.zero_()

        return embeddings
    
    
class Resnet18Model(nn.Module):
    def __init__(self, pretrained=True):
        super(Resnet18Model, self).__init__()
        self.pretrained = pretrained
        self.model = self._load_model()
            
    def _load_model(self):
        # Placeholder for model loading logic
        if self.pretrained:
            print("Loading pretrained ResNet18 model...")
            resnet18_model = models.resnet18(weights='DEFAULT')
        else:
            print("Loading ResNet18 model without pretrained weights...")
            resnet18_model = models.resnet18(weights=None)
        # Replace the final FC layer for binary classification (single output)
        resnet18_model.fc = nn.Linear(resnet18_model.fc.in_features, 1)
        return resnet18_model
    
    def forward(self, x):
        return self.model(x)
    
    def gradient_embedding(self, x):
        x = x.clone().detach().requires_grad_(True)
        logits = self.forward(x)
        pseudo_labels = (logits > 0).float()
        loss = F.binary_cross_entropy_with_logits(logits, pseudo_labels, reduction='sum')
        loss.backward()
        embeddings = x.grad.view(x.size(0), -1).cpu().numpy()
        x.grad.zero_()
        return embeddings